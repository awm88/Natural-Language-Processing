# -*- coding: utf-8 -*-
"""final100.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12aV__g6C8o7u6uxBnfxpaf8u3MtciXjh
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import unicodedata
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
from string import punctuation
import contractions
from nltk.corpus import stopwords
import fastai.text.all as ft
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score

data = pd.read_csv('reviews (2).csv', delimiter='\t', index_col=False)

data.columns = ['Skype', 'Rating', 'Date', 'Review']


def sentiment(n):
    if (n < 3):
        return (0)
    elif (n > 3):
        return 2
    else:
        return 1


data['sentiment'] = data['Rating'].apply(sentiment)
data.head()

data.sentiment.value_counts()

index_names = data[data['sentiment'] == 1].index
data.drop(index_names[158:], inplace=True)

data.sentiment.value_counts()

index_names = data[data['sentiment'] == 2].index
data.drop(index_names[158:], inplace=True)

data.sentiment.value_counts()

data = data[['sentiment', 'Review']]
data

data['Review'] = data['Review'].str.replace("[^a-zA-Z]", " ")


data['Review'] = data['Review'].str.lower()


data['Review'] = data['Review'].str.strip()


def remove_punct(text):
    for punctuations in punctuation:
        text = text.replace(punctuations, '')
    return text


data['Review'] = data['Review'].apply(remove_punct)


def remove_accented_chars(text):
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return text


data['Review'] = data['Review'].apply(remove_accented_chars)


def expand_contractions(con_text):
    con_text = contractions.fix(con_text)
    return con_text


data['Review'] = data['Review'].apply(expand_contractions)
nltk.download('stopwords')


stopword_list = set(stopwords.words('english'))
tokenizer = ToktokTokenizer()


def remove_stopwords(text, is_lower_case=False):
    tokens = tokenizer.tokenize(text)
    tokens = [token.strip() for token in tokens]
    if is_lower_case:
        filtered_tokens = [token for token in tokens if token not in stopword_list]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text


data['Review_norm'] = data['Review'].apply(remove_stopwords)
data = data.drop(['Review'], axis=1)
data.head()
train_data, test_data = train_test_split(data, test_size=0.1, random_state=12, stratify=data['sentiment'])
test_data.reset_index(drop=True, inplace=True)
train_data.reset_index(drop=True, inplace=True)

train_data

test_data

batch_size = 25
epoch = 8
seq_length = 120
validation_percentage = 0.20
learning_rate = 0.1
weight_decay = 0.1

fastai_data_train = ft.TextDataLoaders.from_df(train_data,
                                               text_col='Review_norm',
                                               valid_pct=validation_percentage,
                                               is_lm=True,
                                               seq_len=seq_length,
                                               bs=batch_size)

learn_lm = ft.language_model_learner(fastai_data_train,
                                     ft.AWD_LSTM,
                                     metrics=[ft.accuracy, ft.Perplexity()],
                                     wd=weight_decay).to_fp16()


learn_lm.lr_find()

learn_lm.fine_tune(epochs=epoch, base_lr=learning_rate)

learn_lm.unfreeze()
learn_lm.fit_one_cycle(n_epoch=epoch, lr_max=learning_rate/10)

model_name = "fine_tuned"
learn_lm.save_encoder(model_name)

data_cls = ft.TextDataLoaders.from_df(train_data,
                                      text_col='Review_norm',
                                      label_col='sentiment',
                                      valid_pct=validation_percentage,
                                      is_lm=False,
                                      seq_len=seq_length,
                                      bs=batch_size)

learn_cls = ft.text_classifier_learner(data_cls, ft.AWD_LSTM,
                                       drop_mult=0.2,
                                       metrics=ft.accuracy)


learn_cls.load_encoder('fine_tuned')
learn_cls.freeze()

learn_cls.fit_one_cycle(n_epoch=epoch, lr_max=learning_rate)
learn_cls.freeze_to(-2)


learn_cls.fit_one_cycle(n_epoch=epoch, lr_max=learning_rate/10)
learn_cls.freeze_to(-3)

learn_cls.fit_one_cycle(n_epoch=epoch, lr_max=learning_rate/100)

learn_cls.export(fname='fine_tuned.pkl')

fastai_data_test = learn_cls.dls.test_dl(test_data['Review_norm'])

sentiment_pred_test = learn_cls.get_preds(dl=fastai_data_test)

test_data["sentiment_Prediction"] = sentiment_pred_test[0].argmax(dim=-1)

sentiment_pred_test

pd.options.mode.chained_assignment = None  

test_data["sentiment_Prediction"] = sentiment_pred_test[0].argmax(dim=-1)


y_pred = test_data['sentiment_Prediction']
y_true = test_data['sentiment']
conf_matrix_array = confusion_matrix(y_true, y_pred)
conf_mat_df = pd.DataFrame(conf_matrix_array,
                           index=['Real_Negative', 'Real_neutral', 'Real_positive'],
                           columns=['Pred_negative', 'Pred_neutral', 'Pred_positive'])


print(conf_mat_df)


print('Accuracy: ', accuracy_score(y_true, y_pred))
print('F1-score: ', f1_score(y_true, y_pred, average='weighted'))